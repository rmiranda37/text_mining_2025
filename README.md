# ğŸ“Š Text Mining 2025 â€“ Financial Sentiment Classification

This repository presents the final project for the **Text Mining** course of the Masterâ€™s in Data Science and Advanced Analytics â€“ NOVA IMS.

## ğŸ§  Project Objective

The goal is to classify financial tweets into three sentiment categories: **Bearish**, **Bullish**, and **Neutral**, leveraging Natural Language Processing (NLP) techniques. Tweets are noisy by nature and reflect informal language, posing unique challenges for sentiment analysis.

## ğŸ—‚ï¸ Repository Structure
```
text_mining_2025/
â”œâ”€â”€ Dados/ # Raw datasets (excluded from Git)
â”œâ”€â”€ Entrega Final/ # Final notebooks and predictions
â”‚ â”œâ”€â”€ pred_11.csv # Final predictions on test set
â”‚ â”œâ”€â”€ tm_final_11.ipynb # Clean final notebook
â”‚ â””â”€â”€ tm_testes_11.ipynb # Model experimentation
â”œâ”€â”€ InÃªs & Luis & Pedro & Rafael & Rodrigo/ # Individual contributions
â”œâ”€â”€ Project Handout TM 2025.pdf # Official project handout
â””â”€â”€ README.md # Project overview and structure
```

## ğŸ› ï¸ Key Components

- **EDA & Preprocessing**: Exploration of target imbalance, stopword patterns, and character distributions. Cleaning strategies were adapted for different model families.
- **Modeling Approaches**:
  - Classical models (BoW + KNN, TF-IDF + SVM)
  - Neural Networks (Word2Vec + CNN-BiLSTM, GloVe variants)
  - Transformer-based encoders (RoBERTa, DeBERTa, BERTweet)
- **Model Evaluation**: Metrics include accuracy and macro F1-score. RoBERTa-Large was selected as the final model after fine-tuning with Optuna.
- **Streamlit App**: A lightweight app was built to test the final model in real-time and perform batch evaluations.  
  ğŸ‘‰ [Try it here](https://textminingproject.streamlit.app/)

## ğŸš€ Results

- **Best Transformer**: RoBERTa-Large
- **Validation Metrics**:  
  - Accuracy: **91%**  
  - Macro F1-Score: **0.89**

## ğŸ”‘ Keywords

NLP Â· Sentiment Analysis Â· Financial Texts Â· Transformer Models Â· RoBERTa Â· Text Classification Â· Streamlit Â· BERT Â· Tokenization
